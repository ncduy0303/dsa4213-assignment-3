program: train.py
method: grid
metric:
  name: eval/accuracy
  goal: maximize
parameters:
  # --- Hyperparameters to Sweep ---
  learning_rate:
    values: [2e-5, 2e-4, 2e-3, 2e-2]
  lora_r:
    values: [2, 8, 32, 128]
  lora_target_modules:
    values:
      - "query,value"
      - "query,key,value,attention.output.dense"
      - "query,key,value,attention.output.dense,intermediate.dense,output.dense"

  # --- Fixed Parameters ---
  seed:
    value: 23
  peft_method:
    value: "lora"
  lora_alpha:
    value: 16
  lora_dropout:
    value: 0.1
  lora_bias:
    value: "none"
  model_name_or_path:
    value: "roberta-base"
  dataset_name:
    value: "dair-ai/emotion"
  metric_name:
    value: "accuracy"
  text_column_name:
    value: "text"
  label_column_name:
    value: "label"
  max_seq_length:
    value: 128 # This is enough for the emotion dataset using BERT tokenizer
  shuffle_train_dataset:
    value: true
  do_train:
    value: true
  do_eval:
    value: true
  do_predict:
    value: true
  per_device_train_batch_size:
    value: 64
  num_train_epochs:
    value: 5
  output_dir:
    value: "./outputs/lora"
  save_total_limit:
    value: 2 # Only keep best + last checkpoint
  eval_strategy:
    value: "epoch"
  save_strategy:
    value: "epoch"
  load_best_model_at_end:
    value: true
  metric_for_best_model:
    value: "accuracy"
  report_to:
    value: "wandb"
